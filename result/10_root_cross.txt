There are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation. Other approaches have used topic information, Latent Semantic Analysis (LSA), Sequence to Sequence models, Reinforcement Learning and Adversarial processes. In general, there are two different approaches for automatic summarization: extraction and abstraction. Extractive summarization works by identifying important sections of the text cropping out and stitching together portions of the content to produce a condensed version. Thus, they depend only on the extraction of sentences from the original text. Constructs an intermediate representation of the input text intending to find salient content. Abstractive methods take advantage of recent developments in deep learning. These models consist of an encoder and a decoder, where a neural network reads the text, encodes it, and then generates target text. Thus, they are still far away from reaching human-level quality in summary generation, despite recent progress using neural networks inspired by the progress of neural machine translation and sequence to sequence models. Example output of the attention-based summarization of Alexander et al.
